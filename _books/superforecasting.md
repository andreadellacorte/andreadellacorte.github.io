---
layout: book
title: Superforecasting
description: The Art & Science of Prediction
---

<blockquote>by Philip Tetlock and Dan Gardners</blockquote>

This book describes the practices and skills Superforecasters, a 2% of the world population that can show remarkable ability at predicting future events.

### Chapter 1: An Optimistic Skeptic

Forecasting is common: all humans use this skill to predict things like whether the sun will rise and set, at what time will it do so, or what will happen to the world economy.

Unfortunately, as there are no clear rules on how forecasting should be done and evaluated (especially for complex system and events far in the future), most predictions, even the ones done by experts, can seem as good as 'a chimp throwing darts at a board'.

This was the outcome of a study published in 2011 by Philip Tetlock, the result of a 20+ year study involving 284 forecasters.

Yet, amongst these forecasters, some displayed remarkable accuracy in their predictions; they were the Superforecasters, people who absorb knowledge from many sources, crunch it again and again and ; most of the time they'll be right.

### Chapter 2: The Illusion of Knowledge

Daniel Kahneman, author of "Thinking, Fast and Slow", proposed a "Two Systems" model for human brains; whenever a decision has to be made, the following systems act:

1. System 1 acts first and quickly; it's what keeps us alive in dangerous situation when acting heuristically / instinctively is necessary;
2. System 2 comes into play later; System 2 evaluates the decision taken by System 1 and, if it doesn't hold to judgement, forbids it.

#### Tip-of-nose-perspective

System 1 is a kind of tip-of-your-nose-perspective; its decision making process shows the following issues:

- Confirmation Bias: When reconciling System 1 and System 2 seems impossible, humans can grab on to the first plausible explanation without fact checking it. This "suspension of disbelief" is called confirmation bias; a poor way to make sense of a complex world, but a superb way to satisfy our brain's desire for order.
- Bait and Switch: When faced with a very hard question that can't be answered without more data, humans tend to replace it with a simpler one. This can cause us to jump to conclusions.

System 1 also has benefits:

- Snap Judgement: Learning from past experiences, System 1 can provide creative solutions and intuitions with just a few cues, a way of reasoning that would be beyond the reach of System 2 calculations.

Together, the pros and cons above mean that this tip-of-your-nose-perspective intuitions can fail as spectacularly as succeed. These issues can be mitigated by:

- Recognising whether we are operating an environment with enough valid cues (e.g. a house on fire) or not (e.g. the stock market);
- Double-checking our intuitions again and again, when we have time to make a decision.

#### Medicine and Forecasting: Unscientific

Randomized medicine trials are today common and seem stunningly obvious.

Yet, before the 20th century they were the exception, not the rule: the doctors' inclination to believing their treatments were working, meant strong evidence and rigorous experimentation were unthinkable. A clear example of System 1 Confirmation Bias.

Medicine had never been scientific, due to the lack of doubt in the doctor's mind; in a way, doubt it's what propels science forward.

"Doubt is not a fearful thing," the physicist Richard Feynman observed, "but a thing of very great value. (...) it is of paramount important, in order to make progress, that we recognise ignorance and doubt. Because we have the doubt, we then propose looking in new directions for new ideas."

Similarly, there is today little science, let alone experimentation in forecasting; the outcomes of these bad forecasts can be harder to pinpoint, but our world perspectives are led by them astray to missed opportunities, monetary losses, even death and war.

### Chapter 2: Keeping Score

To find the best forecasters, we need to objectively squeeze out as much ambiguity as humanly possible in our evaluations.

#### Judging Forecasts

Forecasts need to provide clarity on the expected outcomes and their timelines.

In most cases, forecasts are presented without either; this way, whenever forecasts are made, they are impossible to evaluate, making forecasters unaccountable whenever their prediction results wrong.

Forecasters can this way stretch timelines and probability estimates to one side or another, making the prediction work for them, a "wrong-side-of-maybe" fallacy.

Sherman Kent recommends the following language when dealing with estimates:

- 100%: Certain
- 93% (+- 6%): Almost certain
- 75% (+- 12%): Probable
- 50% (+- 10%): Chances about even
- 30% (+- 10%): Probably not
- 7% (+- 5%): Almost certainly not
- 0%: Impossible

This enables estimators to "think about thinking", a process known as metacognition, making the forecasters better at distinguishing finer degrees of uncertainty, but can be met with many cultural difficulties.

#### Judging Forecasters

Since we can't repeat history, it's impossible for us to judge forecasters on one probabilistic forecast only; we'll need to evaluate them on many probabilistic experiments, evaluating their "calibration".

The [Brier score](https://en.wikipedia.org/wiki/Brier_score) is a score function measure for the accuracy of probabilistic predictions, providing better, lower scores to more accurate and confident evaluators; 0.0 representing omniscient knowledge and 0.5 monkey-like luck.

Yet, the goodness of a Brier score depends on the difficulty of the forecasts; a Brier score of 0.2 can be very good for a forecaster predicting very complex forecasts correctly, or very bad for one predicting obvious outcomes wrong. A level playing ground is required.

#### Hedgehogs and Foxes

Over the next 20 years, Philip Tetlock worked with a team of 284 professionals to get their predictions on roughly twenty-eight thousand forecast questions, that ranged from one to five to ten years out. The findings were published in the a study named [Expert Political Judgment](http://emilkirkegaard.dk/en/wp-content/uploads/Philip_E._Tetlock_Expert_Political_Judgment_HowBookos.org_.pdf)(EPJ).

In the results, two spectrums of forecasters could be recognised:

- Hedgehogs: these were forecasters who organised their thinking around one Big Idea; they would advance their analysis with piles of reasons on why they were right and others were wrong and, committed to their conclusions, they'd reluctant to changing their mind, even when clearly wrong.
- Foxes: these were forecasters who drew on many analytical tools, gathering information from as many sources as possible, aggregating it and synthesizing it in one conclusion.

As mentioned above, this is a spectrum: in most cases nobody acted purely as a hedgehog or a fox; how people actually behaved depended a lot on the situation and forecasting question.

Yet, the EPJ study provides us with real insights into what an effective forecasters does.

! Reading In Progress !
